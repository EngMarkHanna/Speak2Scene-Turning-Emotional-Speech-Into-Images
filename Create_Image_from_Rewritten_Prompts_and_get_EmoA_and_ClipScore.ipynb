{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4h_DOJUBzbQ"
      },
      "source": [
        "#Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdBNNyGgnPq7"
      },
      "outputs": [],
      "source": [
        "!pip install torchcodec --quiet\n",
        "!pip install torchinfo --quiet\n",
        "!pip install transformers accelerate sentencepiece torchaudio diffusers datasets soundfile pillow --quiet\n",
        "!pip install rouge-score\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz54_9LJHzXY"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7_UoTPcHy__"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import soundfile as sf\n",
        "from IPython.display import display\n",
        "\n",
        "import clip\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset, Dataset\n",
        "\n",
        "from PIL import Image\n",
        "from diffusers import FluxPipeline\n",
        "from transformers import WhisperProcessor, WhisperModel, WhisperForConditionalGeneration, pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "import gc\n",
        "import json\n",
        "import uuid\n",
        "import time\n",
        "import random\n",
        "import librosa\n",
        "import logging\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from sklearn.svm import LinearSVC\n",
        "from collections import defaultdict\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58AbwdkehJKO"
      },
      "source": [
        "#Create the images from the rewritten LLM prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate pictures to use and classify for EmoA from rewritten prompts"
      ],
      "metadata": {
        "id": "GGZFNnaZr3lN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoBERgLfhMq9"
      },
      "outputs": [],
      "source": [
        "class ImageGeneration(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        flux_model_name=\"black-forest-labs/FLUX.1-schnell\",\n",
        "        device=\"cuda\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        self.flux = FluxPipeline.from_pretrained(\n",
        "            flux_model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=device\n",
        "        )\n",
        "        self.flux.set_progress_bar_config(disable=True)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        prompt,\n",
        "        flux_height=512,\n",
        "        flux_width=512,\n",
        "        flux_steps=40,\n",
        "        flux_guidance=3.0,\n",
        "        flux_seed=0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Forward pass (batched):\n",
        "        prompt(s) -> FLUX pipeline â†’ image(s)\n",
        "\n",
        "        prompt: str or List[str]\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(prompt, str):\n",
        "            prompt = [prompt]\n",
        "\n",
        "        batch_size = len(prompt)\n",
        "\n",
        "\n",
        "        generator = torch.Generator(\"cpu\").manual_seed(flux_seed)\n",
        "\n",
        "\n",
        "        out = self.flux(\n",
        "            prompt,\n",
        "            height=flux_height,\n",
        "            width=flux_width,\n",
        "            guidance_scale=flux_guidance,\n",
        "            num_inference_steps=flux_steps,\n",
        "            generator=generator,\n",
        "        )\n",
        "\n",
        "        images = out.images\n",
        "\n",
        "        return {\n",
        "            \"Prompt\": prompt,\n",
        "            \"images\": images,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FDTeK75mUiE"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/GenAI/Project/modified_prompts.json\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRFYGc4vmN3y"
      },
      "outputs": [],
      "source": [
        "def clean_prompt(p):\n",
        "\n",
        "    p = p.replace(\"<|endoftext|>\", \" \")\n",
        "\n",
        "\n",
        "    p = \" \".join(p.split())\n",
        "\n",
        "\n",
        "    words = p.split()\n",
        "    if len(words) > 70:\n",
        "        words = words[:70]\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "vit_classes = {'disgust': 0, 'fear': 1, 'happy': 2, 'sad': 3, 'surprised': 4}\n",
        "emotion_names = list(vit_classes.keys())\n",
        "\n",
        "\n",
        "with open(\"/content/modified_prompts.json\", \"r\") as f:\n",
        "    rewritten_prompts = json.load(f)\n",
        "\n",
        "data = []\n",
        "for item in rewritten_prompts:\n",
        "  prompt = clean_prompt(item[\"new_prompt\"])\n",
        "  emo = item[\"emotion\"]\n",
        "  data.append((prompt, emo))\n",
        "\n",
        "batch_size = 1\n",
        "test_ds_ImgGen = data\n",
        "test_loader = DataLoader(test_ds_ImgGen, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Total samples:\", len(test_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AchCTsSvnCfl"
      },
      "outputs": [],
      "source": [
        "image_model = ImageGeneration().to(\"cuda\")\n",
        "image_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "def compute_clip_score(pil_image, prompt):\n",
        "    tokens = prompt.split()\n",
        "    if len(tokens) > 70:\n",
        "        prompt = \" \".join(tokens[:70])\n",
        "\n",
        "    image_tensor = clip_preprocess(pil_image).unsqueeze(0).to(device)\n",
        "    text_tensor = clip.tokenize([prompt], truncate=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_feat = clip_model.encode_image(image_tensor)\n",
        "        txt_feat = clip_model.encode_text(text_tensor)\n",
        "        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
        "        txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return (img_feat @ txt_feat.T).item()"
      ],
      "metadata": {
        "id": "K_cmLZrHsXeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az9lLDKamNyq"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/Images\n",
        "save_root = \"/content/Images\"\n",
        "os.makedirs(save_root, exist_ok=True)\n",
        "\n",
        "for emo in emotion_names:\n",
        "    os.makedirs(os.path.join(save_root, emo), exist_ok=True)\n",
        "\n",
        "all_samples = []\n",
        "\n",
        "\n",
        "for batch in tqdm(test_loader):\n",
        "    prompts, emotions = batch\n",
        "\n",
        "\n",
        "    out = image_model(\n",
        "        prompt=list(prompts),\n",
        "        flux_height=512,\n",
        "        flux_width=512,\n",
        "        flux_steps=30,\n",
        "        flux_guidance=3.0,\n",
        "        flux_seed=0,\n",
        "    )\n",
        "\n",
        "    images = out[\"images\"]\n",
        "\n",
        "    for prompt, emo, img in zip(prompts, emotions, images):\n",
        "        clip_score = compute_clip_score(img, prompt)\n",
        "        all_samples.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"emotion\": emo,\n",
        "            \"image\": img,\n",
        "            \"clip_score\": clip_score\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uglE7aX7UcZ"
      },
      "outputs": [],
      "source": [
        "os.makedirs(save_root, exist_ok=True)\n",
        "\n",
        "\n",
        "for emo in emotion_names:\n",
        "    os.makedirs(os.path.join(save_root, emo), exist_ok=True)\n",
        "\n",
        "counters = defaultdict(int)\n",
        "\n",
        "for sample in all_samples:\n",
        "    emo = str(sample[\"emotion\"])\n",
        "    img = sample[\"image\"]\n",
        "\n",
        "    if not hasattr(img, \"save\"):\n",
        "        raise TypeError(f\"Image for emotion '{emo}' is not a PIL Image\")\n",
        "\n",
        "\n",
        "    idx = counters[emo]\n",
        "    counters[emo] += 1\n",
        "\n",
        "    filename = f\"{emo}_{idx:05d}.png\"\n",
        "    save_path = os.path.join(save_root, emo, filename)\n",
        "\n",
        "\n",
        "    img.save(save_path)\n",
        "\n",
        "    sample[\"image\"] = save_path\n",
        "\n",
        "\n",
        "json_path = os.path.join(save_root, \"metadata.json\")\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(all_samples, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Saved metadata to {json_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-0MPPmhLbwZ"
      },
      "outputs": [],
      "source": [
        "len(all_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsIxEaT55fa6"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"/content/Images\", \"zip\", \"/content/Images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdxCZ8Ghd5cr"
      },
      "source": [
        "#EmoA evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model, prepare dataset loader and test."
      ],
      "metadata": {
        "id": "U2sdBY1przlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q -o \"/content/drive/MyDrive/GenAI/Project/Images.zip\" -d \"/content/data\""
      ],
      "metadata": {
        "id": "62KO-2tVrhpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDZdBbM7O6Kx"
      },
      "outputs": [],
      "source": [
        "!find /content/data -maxdepth 3 -type d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpfF-qwVO-4O"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/data/.ipynb_checkpoints\n",
        "!rm -rf \"/content/data/angry\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83WvGwfQNPTz"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "\n",
        "vit_classes = {'disgust': 0, 'fear': 1, 'happy': 2, 'sad': 3, 'surprised': 4}\n",
        "\n",
        "\n",
        "test_dir = \"/content/data\" #Maybe need change based on output path\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_ds = datasets.ImageFolder(test_dir, transform=val_test_transform)\n",
        "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
        "\n",
        "class_names = list(test_ds.class_to_idx.keys())\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Samples:\", len(test_ds))\n",
        "\n",
        "\n",
        "vit_path = \"/content/drive/MyDrive/GenAI/Project/vit_swag_best_val (3).pt\"\n",
        "\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "\n",
        "weights = ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\n",
        "model = vit_b_16(weights=weights)\n",
        "\n",
        "in_features = model.heads.head.in_features\n",
        "model.heads.head = nn.Linear(in_features, len(vit_classes))\n",
        "\n",
        "\n",
        "checkpoint = torch.load(vit_path, map_location=device)\n",
        "state = checkpoint[\"model_state_dict\"]\n",
        "model.load_state_dict(state)\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "tot_loss = tot_correct = tot = 0\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "for x, y in test_loader:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "\n",
        "    pred = out.argmax(1)\n",
        "\n",
        "    tot_loss += loss.item() * x.size(0)\n",
        "    tot_correct += (pred == y).sum().item()\n",
        "    tot += y.size(0)\n",
        "\n",
        "    all_preds.extend(pred.cpu().numpy())\n",
        "    all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "avg_loss = tot_loss / tot\n",
        "acc = tot_correct / tot\n",
        "\n",
        "print(f\"\\nTest Loss: {avg_loss:.4f} | Accuracy: {acc:.4f}\\n\")\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CLIP Score Evaluation"
      ],
      "metadata": {
        "id": "oilevCUusBAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Clipscore for every emotion and check global average"
      ],
      "metadata": {
        "id": "Ra4FhftJvsA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q -o \"/content/Images.zip\" -d \"/content/data\""
      ],
      "metadata": {
        "id": "Ehw_ih5Es4Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_classes = {'disgust': 0, 'fear': 1, 'happy': 2, 'sad': 3, 'surprised': 4}\n",
        "\n",
        "\n",
        "with open(\"/content/data/metadata.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "\n",
        "emotion_sums = {emo: 0.0 for emo in vit_classes}\n",
        "emotion_counts = {emo: 0   for emo in vit_classes}\n",
        "\n",
        "\n",
        "for entry in data:\n",
        "    emo = entry[\"emotion\"]\n",
        "    score = entry[\"clip_score\"]\n",
        "\n",
        "    if emo in vit_classes:\n",
        "        emotion_sums[emo] += score\n",
        "        emotion_counts[emo] += 1\n",
        "\n",
        "emotion_averages = {\n",
        "    emo: (emotion_sums[emo] / emotion_counts[emo]\n",
        "          if emotion_counts[emo] > 0 else None)\n",
        "    for emo in vit_classes\n",
        "}\n",
        "\n",
        "\n",
        "all_scores = [\n",
        "    entry[\"clip_score\"]\n",
        "    for entry in data\n",
        "    if entry[\"emotion\"] in vit_classes\n",
        "]\n",
        "\n",
        "global_average = sum(all_scores) / len(all_scores) if all_scores else None\n",
        "\n",
        "\n",
        "print(\"Average CLIP score per emotion:\")\n",
        "for emo, avg in emotion_averages.items():\n",
        "    print(f\"{emo}: {avg:.4f}\" if avg is not None else f\"{emo}: No samples\")\n",
        "\n",
        "print(\"\\nOverall average CLIP score:\")\n",
        "print(global_average)\n"
      ],
      "metadata": {
        "id": "LBZC0ggWsCoz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "lz54_9LJHzXY"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}