{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4h_DOJUBzbQ"
      },
      "source": [
        "#Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdBNNyGgnPq7"
      },
      "outputs": [],
      "source": [
        "!pip install torchcodec --quiet\n",
        "!pip install torchinfo --quiet\n",
        "!pip install transformers accelerate sentencepiece torchaudio diffusers datasets soundfile pillow --quiet\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz54_9LJHzXY"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7_UoTPcHy__"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import soundfile as sf\n",
        "from IPython.display import display\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "from torchvision import datasets\n",
        "from torchvision.models import vit_b_16\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset, Dataset\n",
        "\n",
        "from PIL import Image\n",
        "from diffusers import FluxPipeline\n",
        "from transformers import WhisperProcessor, WhisperModel, WhisperForConditionalGeneration, pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "\n",
        "import gc\n",
        "import json\n",
        "import uuid\n",
        "import time\n",
        "import random\n",
        "import librosa\n",
        "import logging\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from sklearn.svm import LinearSVC\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qb0UY-thCx5"
      },
      "source": [
        "#Rewrite Prompts LLM part alone"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rewrite 5000 prompts from text2image dataset to use them for Image generation later.\n",
        "\n",
        "Also Used for Rouge evaluation."
      ],
      "metadata": {
        "id": "yABRucpOnptv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHSxWd1wZnUk"
      },
      "outputs": [],
      "source": [
        "#Took 5000 prompts randomely from text2image dataset\n",
        "!cp \"/content/drive/MyDrive/GenAI/Project/text2image_5000.jsonl\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlZy-bILj3ax"
      },
      "outputs": [],
      "source": [
        "#We picked these emotions because later we can do emoA on them\n",
        "vit_classes = {\n",
        "   'disgust': 0, 'fear': 1,\n",
        "    'happy': 2, 'sad': 3, 'surprised': 4\n",
        "}\n",
        "emotion_names = list(vit_classes.keys())\n",
        "\n",
        "\n",
        "prompts = []\n",
        "with open(\"/content/drive/MyDrive/GenAI/Project/text2image_5000.jsonl\", \"r\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            prompts.append(json.loads(line))\n",
        "\n",
        "# Build labeled dataset (text + random emotion only)\n",
        "labeled_data = []\n",
        "for item in prompts:\n",
        "    text = item[\"text\"]\n",
        "    emotion = random.choice(emotion_names)\n",
        "\n",
        "    labeled_data.append({\n",
        "        \"text\": text,\n",
        "        \"emotion\": emotion\n",
        "    })\n",
        "\n",
        "with open(\"/content/labeled_prompts.json\", \"w\") as f:\n",
        "    json.dump(labeled_data, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgNu_4-rxZdk"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "system_prompt = (\n",
        "\"\"\"\n",
        "Your task is to modify the given prompt by inserting emotion-related adjectives\n",
        "while preserving every original word.\n",
        "\n",
        "Rules:\n",
        "1. Do NOT delete, replace, or reorder any original words.\n",
        "2. Only INSERT 1–2 adjectives related to the given emotion.\n",
        "3. Adjectives must be inserted directly before nouns.\n",
        "4. Do NOT add new sentences, explanations, or details.\n",
        "5. Output only the modified prompt — nothing else.\n",
        "\n",
        "Format:\n",
        "Original prompt: <prompt>\n",
        "Emotion: <emotion>\n",
        "Output: <modified prompt>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "user_template = (\n",
        "\"\"\"\n",
        "Original prompt: {prompt}\n",
        "Emotion: {emotion}\n",
        "Output:\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "class LLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        llm_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        device=\"cuda\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
        "            llm_model_name,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map={\"\": 0}\n",
        "        )\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name, use_fast=True)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        prompt,\n",
        "        emotion,\n",
        "        max_new_tokens=40,\n",
        "        temperature=0.6,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        stop_strings=None\n",
        "    ):\n",
        "\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_template.format(prompt=prompt, emotion=emotion).strip()},\n",
        "        ]\n",
        "\n",
        "\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.llm.device)\n",
        "\n",
        "        # FIX: proper attention mask\n",
        "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.llm.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                top_k=top_k,\n",
        "                do_sample=do_sample,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "\n",
        "        gen_ids = output_ids[0, input_ids.shape[1]:]\n",
        "        text = self.tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "        # Cleanup mostly for other reasoning and non reasoning models that we used\n",
        "        if \"</think>\" in text:\n",
        "            text = text.split(\"</think>\", 1)[-1].strip()\n",
        "\n",
        "\n",
        "        bad_prefixes = [\n",
        "            \"Original prompt:\", \"Output:\", \"Modified prompt:\", \"modified prompt:\",\n",
        "            \"Rewritten Prompt:\", \"Prompt:\", \"Result:\"\n",
        "        ]\n",
        "        for p in bad_prefixes:\n",
        "            if p in text:\n",
        "                text = text.split(p)[-1].strip()\n",
        "\n",
        "\n",
        "        if \"Original prompt:\" in text and \"Emotion:\" in text:\n",
        "\n",
        "            if \"Output:\" in text:\n",
        "                text = text.split(\"Output:\")[-1].strip()\n",
        "\n",
        "\n",
        "        text = text.strip().strip('\"').strip()\n",
        "\n",
        "        return {\n",
        "            \"input_prompt\": prompt,\n",
        "            \"emotion\": emotion,\n",
        "            \"raw_output\": text,\n",
        "            \"final_prompt\": text\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = LLM().to(\"cuda\")\n",
        "llm_model.eval()"
      ],
      "metadata": {
        "id": "nNmvQGRXo6sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFovePNZaYOn"
      },
      "outputs": [],
      "source": [
        "#Take all items for now\n",
        "subset = labeled_data\n",
        "\n",
        "\n",
        "prompts = [item[\"text\"] for item in subset]\n",
        "emotions = [item[\"emotion\"] for item in subset]\n",
        "\n",
        "\n",
        "new_prompts = []\n",
        "unfiltered = []\n",
        "\n",
        "for original, emo in tqdm(zip(prompts, emotions), total=len(prompts), desc=\"Generating prompts\"):\n",
        "    result = llm_model(\n",
        "        prompt=original,\n",
        "        emotion=emo,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.0,\n",
        "        do_sample=False\n",
        "    )\n",
        "    new_prompts.append(result[\"final_prompt\"])\n",
        "    unfiltered.append(result[\"raw_output\"])\n",
        "\n",
        "\n",
        "output_data = []\n",
        "\n",
        "for original, emotion, new in zip(prompts, emotions, new_prompts):\n",
        "    output_data.append({\n",
        "        \"original_prompt\": original,\n",
        "        \"emotion\": emotion,\n",
        "        \"new_prompt\": new\n",
        "    })\n",
        "\n",
        "\n",
        "with open(\"/content/modified_prompts.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "print(\"Saved to modified_prompts.json\")\n",
        "\n",
        "\n",
        "for original, emotion, new in zip(prompts, emotions, new_prompts):\n",
        "    print(f\"Original prompt: {original}\")\n",
        "    print(f\"Emotion: {emotion}\")\n",
        "    print(f\"New prompt: {new}\")\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Eval Rewritten Prompts Using Rouge Score"
      ],
      "metadata": {
        "id": "ez5Zo67So_73"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--s9_B6MYNSo"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open(\"/content/modified_prompts.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "\n",
        "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "results = []\n",
        "\n",
        "for item in data:\n",
        "    original = item[\"original_prompt\"]\n",
        "    rewritten = item[\"new_prompt\"]\n",
        "\n",
        "    scores = rouge.score(original, rewritten)\n",
        "\n",
        "    results.append({\n",
        "        \"original_prompt\": original,\n",
        "        \"emotion\": item[\"emotion\"],\n",
        "        \"new_prompt\": rewritten,\n",
        "        \"rouge1\": scores[\"rouge1\"].fmeasure,\n",
        "        \"rouge2\": scores[\"rouge2\"].fmeasure,\n",
        "        \"rougeL\": scores[\"rougeL\"].fmeasure\n",
        "    })\n",
        "\n",
        "# Compute averages\n",
        "avg_rouge1 = sum(r[\"rouge1\"] for r in results) / len(results)\n",
        "avg_rouge2 = sum(r[\"rouge2\"] for r in results) / len(results)\n",
        "avg_rougeL = sum(r[\"rougeL\"] for r in results) / len(results)\n",
        "\n",
        "print(\"=== AVERAGE ROUGE SCORES ===\")\n",
        "print(\"ROUGE-1:\", avg_rouge1)\n",
        "print(\"ROUGE-2:\", avg_rouge2)\n",
        "print(\"ROUGE-L:\", avg_rougeL)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1VfJErflGtnN",
        "wwm70ck1Aldr",
        "_Qb0UY-thCx5",
        "58AbwdkehJKO",
        "EuCL4KICk-hF"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}