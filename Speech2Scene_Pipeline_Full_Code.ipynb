{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4h_DOJUBzbQ"
      },
      "source": [
        "#Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdBNNyGgnPq7"
      },
      "outputs": [],
      "source": [
        "!pip install torchcodec --quiet\n",
        "!pip install torchinfo --quiet\n",
        "!pip install transformers accelerate sentencepiece torchaudio diffusers datasets soundfile pillow --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz54_9LJHzXY"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7_UoTPcHy__"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import soundfile as sf\n",
        "from IPython.display import display\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "from torchvision import datasets\n",
        "from torchvision.models import vit_b_16\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset, Dataset\n",
        "\n",
        "from PIL import Image\n",
        "from diffusers import FluxPipeline\n",
        "from transformers import WhisperProcessor, WhisperModel, WhisperForConditionalGeneration, pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "\n",
        "import gc\n",
        "import json\n",
        "import uuid\n",
        "import time\n",
        "import random\n",
        "import librosa\n",
        "import logging\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VfJErflGtnN"
      },
      "source": [
        "#Dataset Preparation (just for inference we need the audio files)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the mixed dataset (we merged) From Kaggle, unzip it and copy it to /content/.\n",
        "\n",
        "Check and delete corrupted files.\n",
        "Setup dataloaders and preprocess data, make it compatible with Whisper."
      ],
      "metadata": {
        "id": "MCtyKpJ1chAV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "B1XOjZ9gGqp9"
      },
      "outputs": [],
      "source": [
        "path = kagglehub.dataset_download(\"kamilhanna/emotion-dataset\", force_download=True)\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRWGmOTdtbbZ"
      },
      "outputs": [],
      "source": [
        "!mv \"{path}\" \"/content\"\n",
        "!mv \"/content/1/content/Emotion\" \"/content/\"\n",
        "!rm -rf \"/content/1\"\n",
        "\n",
        "# #Dropping this class cause low data\n",
        "!rm -rf \"/content/Emotion/calm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQILCMPBSG2X"
      },
      "outputs": [],
      "source": [
        "#Detete corrupted files\n",
        "def clean_corrupted_audio(root):\n",
        "    removed = 0\n",
        "    checked = 0\n",
        "\n",
        "    print(f\"Scanning audio under: {root}\\n\")\n",
        "\n",
        "    for folder, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if not f.lower().endswith((\".wav\", \".m4a\")):\n",
        "                continue\n",
        "\n",
        "            path = os.path.join(folder, f)\n",
        "            checked += 1\n",
        "\n",
        "            try:\n",
        "                audio, sr = torchaudio.load(path)\n",
        "            except Exception as e:\n",
        "                print(f\"[CORRUPTED] Removing: {path}   -->   {e}\")\n",
        "                try:\n",
        "                    os.remove(path)\n",
        "                    removed += 1\n",
        "                except Exception as re:\n",
        "                    print(f\"[ERROR] Could not delete {path}: {re}\")\n",
        "                continue\n",
        "\n",
        "    print(\"\\n===== SUMMARY =====\")\n",
        "    print(f\"Checked:  {checked} files\")\n",
        "    print(f\"Removed:  {removed} corrupted files\\n\")\n",
        "    return removed\n",
        "\n",
        "clean_corrupted_audio(\"/content/Emotion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwm70ck1Aldr"
      },
      "source": [
        "#FULL SPEECH 2 SCENE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get models .pt file for MLP\n",
        "\n",
        "Setup class for all different modules"
      ],
      "metadata": {
        "id": "XWrpy5JGj2Yz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWrYNEtGOU9a"
      },
      "outputs": [],
      "source": [
        "!cp  \"/content/drive/MyDrive/GenAI/Project/best_model.pt\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9_0W7EeLMDd"
      },
      "outputs": [],
      "source": [
        "MODEL_HIDDEN_DIMENSION = 1280\n",
        "num_classes = 8\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_1 = 1024\n",
        "        hidden_2 = 512\n",
        "        hidden_3 = 256\n",
        "        hidden_4 = 128\n",
        "        hidden_5 = 64\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(MODEL_HIDDEN_DIMENSION, MODEL_HIDDEN_DIMENSION * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.15),\n",
        "            nn.BatchNorm1d(MODEL_HIDDEN_DIMENSION * 2),\n",
        "\n",
        "            nn.Linear(MODEL_HIDDEN_DIMENSION * 2, hidden_1),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.15),\n",
        "            nn.BatchNorm1d(hidden_1),\n",
        "\n",
        "            nn.Linear(hidden_1, hidden_2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.15),\n",
        "            nn.BatchNorm1d(hidden_2),\n",
        "\n",
        "            nn.Linear(hidden_2, hidden_3),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.15),\n",
        "            nn.BatchNorm1d(hidden_3),\n",
        "\n",
        "            nn.Linear(hidden_3, hidden_3),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.15),\n",
        "            nn.BatchNorm1d(hidden_3),\n",
        "\n",
        "            nn.Linear(hidden_3, hidden_4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.15),\n",
        "            nn.BatchNorm1d(hidden_4),\n",
        "\n",
        "            nn.Linear(hidden_4, hidden_5),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.15),\n",
        "            nn.BatchNorm1d(hidden_5),\n",
        "\n",
        "            nn.Linear(hidden_5, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "#Put in Cuda later\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model_mlp = EmotionClassifier(num_classes).to(device)\n",
        "model_mlp.load_state_dict(torch.load(\"/content/best_model.pt\"))\n",
        "\n",
        "summary(model_mlp, input_size=(1, MODEL_HIDDEN_DIMENSION))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Map MLP output to emotion\n",
        "IDX_TO_CLASS = {\n",
        "    0: \"angry\",\n",
        "    1: \"contempt\",\n",
        "    2: \"disgust\",\n",
        "    3: \"fear\",\n",
        "    4: \"happy\",\n",
        "    5: \"neutral\",\n",
        "    6: \"sad\",\n",
        "    7: \"surprised\"\n",
        "}\n",
        "\n",
        "#LLM prompt\n",
        "system_prompt = (\n",
        "\"\"\"\n",
        "Your task is to modify the given prompt by inserting emotion-related adjectives\n",
        "while preserving every original word.\n",
        "\n",
        "Rules:\n",
        "1. Do NOT delete, replace, or reorder any original words.\n",
        "2. Only INSERT 1–2 adjectives related to the given emotion.\n",
        "3. Adjectives must be inserted directly before nouns.\n",
        "4. Do NOT add new sentences, explanations, or details.\n",
        "5. Output only the modified prompt — nothing else.\n",
        "\n",
        "Format:\n",
        "Original prompt: <prompt>\n",
        "Emotion: <emotion>\n",
        "Output: <modified prompt>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "user_template = (\n",
        "\"\"\"\n",
        "Original prompt: {prompt}\n",
        "Emotion: {emotion}\n",
        "Output:\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "#Gpu timing\n",
        "def measure_gpu_time(fn, *args, **kwargs):\n",
        "    torch.cuda.synchronize()\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start.record()\n",
        "    output = fn(*args, **kwargs)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    elapsed_ms = start.elapsed_time(end)\n",
        "    return output, elapsed_ms\n",
        "\n",
        "#Whole pipeline module\n",
        "class Speech2SceneModule(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        whisper_model_name=\"openai/whisper-large-v3\",\n",
        "        flux_model_name=\"black-forest-labs/FLUX.1-schnell\",\n",
        "        llm_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        mlp_classifier=None,\n",
        "        device=\"cuda\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        # Whipser transcription\n",
        "        self.processor = WhisperProcessor.from_pretrained(whisper_model_name)\n",
        "        self.whisper = WhisperForConditionalGeneration.from_pretrained(\n",
        "            whisper_model_name\n",
        "        ).to(device)\n",
        "        self.whisper.eval()\n",
        "\n",
        "        # FLux image generation\n",
        "        self.flux = FluxPipeline.from_pretrained(\n",
        "            flux_model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=device\n",
        "        )\n",
        "        self.flux.set_progress_bar_config(disable=True)\n",
        "\n",
        "        # Llama LLM rewriting\n",
        "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
        "            llm_model_name,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map={\"\": 0}\n",
        "        )\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name, use_fast=True)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # MLP emotion Classifier\n",
        "        self.mlp = mlp_classifier.to(device)\n",
        "        self.mlp.eval()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        audio_path,\n",
        "        emotion_to_prompt=True,\n",
        "        flux_height=512,\n",
        "        flux_width=512,\n",
        "        flux_steps=40,\n",
        "        flux_guidance=3.0,\n",
        "        flux_seed=0,\n",
        "        llm_temperature=0.2,\n",
        "        llm_max_new_tokens=100,\n",
        "        llm_top_p=0.9,\n",
        "        llm_top_k=50,\n",
        "        llm_do_sample=True,\n",
        "        stop_strings=None\n",
        "    ):\n",
        "\n",
        "        # load audio\n",
        "        audio, sr = torchaudio.load(audio_path)\n",
        "        if audio.shape[0] > 1:\n",
        "            audio = audio.mean(dim=0)\n",
        "        else:\n",
        "            audio = audio[0]\n",
        "        if sr != 16000:\n",
        "            audio = torchaudio.functional.resample(audio, sr, 16000)\n",
        "\n",
        "        audio = audio.float()\n",
        "\n",
        "        # preprocess audio to become compatible with Whisper\n",
        "        feats = self.processor(\n",
        "            audio.cpu().numpy(),\n",
        "            sampling_rate=16000,\n",
        "            padding=\"max_length\",\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_features = feats.input_features.to(self.device)\n",
        "        attention_mask = feats.attention_mask.to(self.device)\n",
        "\n",
        "        # Emotion classification, Encode Audio, Transcription Generation\n",
        "\n",
        "        # Whisper Encoder timing\n",
        "        (enc_out_dict, whisper_enc_ms) = measure_gpu_time(\n",
        "            self.whisper.model.encoder,\n",
        "            input_features=input_features,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "        enc_out = enc_out_dict.last_hidden_state\n",
        "        pooled = enc_out.mean(dim=1)\n",
        "\n",
        "        # MLP timing\n",
        "        (logits, mlp_ms) = measure_gpu_time(self.mlp, pooled)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        conf, idx = torch.max(probs, dim=-1)\n",
        "        idx = idx.item()\n",
        "        conf = conf.item()\n",
        "        emotion_label = IDX_TO_CLASS[idx]\n",
        "\n",
        "        forced_ids = self.processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
        "\n",
        "        (generated_ids, whisper_dec_ms) = measure_gpu_time(\n",
        "            self.whisper.generate,\n",
        "            input_features=input_features,\n",
        "            attention_mask=attention_mask,\n",
        "            forced_decoder_ids=forced_ids\n",
        "        )\n",
        "\n",
        "        transcript = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "\n",
        "        #LLM rewriting\n",
        "        if emotion_to_prompt:\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_template.format(prompt=transcript, emotion=emotion_label).strip()},\n",
        "            ]\n",
        "\n",
        "            input_ids = self.tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(self.llm.device)\n",
        "\n",
        "            attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
        "\n",
        "            # LLM timing\n",
        "            (output_ids, llm_ms) = measure_gpu_time(\n",
        "                self.llm.generate,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=llm_max_new_tokens,\n",
        "                temperature=llm_temperature,\n",
        "                top_p=llm_top_p,\n",
        "                top_k=llm_top_k,\n",
        "                do_sample=llm_do_sample,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "            gen_ids = output_ids[0, input_ids.shape[1]:]\n",
        "            text = self.tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "            # Cleanup mostly for other reasoning and non reasoning models that we used\n",
        "            if \"</think>\" in text:\n",
        "                text = text.split(\"</think>\", 1)[-1].strip()\n",
        "\n",
        "            if stop_strings:\n",
        "                for s in stop_strings:\n",
        "                    if s in text:\n",
        "                        text = text.split(s)[0].strip()\n",
        "                        break\n",
        "\n",
        "            bad_prefixes = [\n",
        "                \"Original prompt:\", \"Output:\", \"Modified prompt:\", \"modified prompt:\",\n",
        "                \"Rewritten Prompt:\", \"Prompt:\", \"Result:\"\n",
        "            ]\n",
        "            for p in bad_prefixes:\n",
        "                if p in text:\n",
        "                    text = text.split(p)[-1].strip()\n",
        "\n",
        "            if \"Original prompt:\" in text and \"Emotion:\" in text and \"Output:\" in text:\n",
        "                text = text.split(\"Output:\", 1)[-1].strip()\n",
        "\n",
        "            prompt = text.strip().strip('\"').strip()\n",
        "\n",
        "        else:\n",
        "            prompt = transcript\n",
        "            llm_ms = 0.0\n",
        "\n",
        "        # Flux image generation\n",
        "        generator = torch.Generator(\"cpu\").manual_seed(flux_seed)\n",
        "\n",
        "        (flux_output, flux_ms) = measure_gpu_time(\n",
        "            self.flux,\n",
        "            prompt,\n",
        "            height=flux_height,\n",
        "            width=flux_width,\n",
        "            guidance_scale=flux_guidance,\n",
        "            num_inference_steps=flux_steps,\n",
        "            generator=generator\n",
        "        )\n",
        "        img = flux_output.images[0]\n",
        "\n",
        "        #Time of whole pipeline\n",
        "        total_time_ms = whisper_enc_ms + mlp_ms + whisper_dec_ms + llm_ms + flux_ms\n",
        "\n",
        "        return {\n",
        "            \"transcript\": transcript,\n",
        "            \"emotion\": emotion_label,\n",
        "            \"emotion_confidence\": conf,\n",
        "            \"prompt\": prompt,\n",
        "            \"image\": img,\n",
        "\n",
        "            \"timings_ms\": {\n",
        "                \"whisper_encoder\": whisper_enc_ms,\n",
        "                \"mlp\": mlp_ms,\n",
        "                \"whisper_decoder\": whisper_dec_ms,\n",
        "                \"llm\": llm_ms,\n",
        "                \"flux\": flux_ms,\n",
        "                \"total\": total_time_ms\n",
        "            },\n",
        "\n",
        "            \"emotion_probs\": probs.detach().cpu(),\n",
        "            \"embedding\": pooled.detach().cpu(),\n",
        "            \"unfiltered_prompt\": transcript,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "JFSSUcB4GWh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDt-d4cRSfWS"
      },
      "outputs": [],
      "source": [
        "s2s = Speech2SceneModule(mlp_classifier=model_mlp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SAMPLES = 50\n",
        "\n",
        "timings_list = []\n",
        "\n",
        "count = 0\n",
        "path = \"/content/emotion-dataset/content/Emotion\"\n",
        "for folder in os.listdir(path):\n",
        "    folder_path = os.path.join(path, folder)\n",
        "\n",
        "    for file in os.listdir(folder_path):\n",
        "        if count >= MAX_SAMPLES:\n",
        "            break\n",
        "\n",
        "        audio_path = os.path.join(folder_path, file)\n",
        "\n",
        "        result = s2s(audio_path, llm_temperature=0.2)\n",
        "\n",
        "\n",
        "        t = result[\"timings_ms\"]\n",
        "        timings_list.append(t)\n",
        "\n",
        "\n",
        "        print(f\"\\n===== Sample {count+1} =====\")\n",
        "        print(\"Transcript:\", result[\"transcript\"])\n",
        "        print(f\"Emotion: {result['emotion']}  (conf={result['emotion_confidence']:.3f})\")\n",
        "        print(\"Prompt:\", result[\"prompt\"])\n",
        "\n",
        "        print(\"\\n--- Timings (ms) ---\")\n",
        "        print(f\"Whisper Encoder:  {t['whisper_encoder']:.2f}\")\n",
        "        print(f\"MLP:              {t['mlp']:.2f}\")\n",
        "        print(f\"Whisper Decoder:  {t['whisper_decoder']:.2f}\")\n",
        "        print(f\"LLM:              {t['llm']:.2f}\")\n",
        "        print(f\"Flux:             {t['flux']:.2f}\")\n",
        "        print(f\"Total:            {t['total']:.2f}\")\n",
        "\n",
        "        display(result[\"image\"])\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    if count >= MAX_SAMPLES:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "#compute averages\n",
        "avg_whisper_enc = np.mean([x[\"whisper_encoder\"] for x in timings_list])\n",
        "avg_mlp         = np.mean([x[\"mlp\"] for x in timings_list])\n",
        "avg_whisper_dec = np.mean([x[\"whisper_decoder\"] for x in timings_list])\n",
        "avg_llm         = np.mean([x[\"llm\"] for x in timings_list])\n",
        "avg_flux        = np.mean([x[\"flux\"] for x in timings_list])\n",
        "avg_total       = np.mean([x[\"total\"] for x in timings_list])\n",
        "\n",
        "print(\"\\n====================================\")\n",
        "print(\"          AVERAGE TIMINGS (ms)      \")\n",
        "print(\"====================================\")\n",
        "print(f\"Avg Whisper Encoder:  {avg_whisper_enc:.2f} ms\")\n",
        "print(f\"Avg MLP:              {avg_mlp:.2f} ms\")\n",
        "print(f\"Avg Whisper Decoder:  {avg_whisper_dec:.2f} ms\")\n",
        "print(f\"Avg LLM:              {avg_llm:.2f} ms\")\n",
        "print(f\"Avg Flux:             {avg_flux:.2f} ms\")\n",
        "print(f\"Avg TOTAL:            {avg_total:.2f} ms\")\n",
        "print(\"====================================\")\n"
      ],
      "metadata": {
        "id": "NGK2g0YOGn3v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1VfJErflGtnN",
        "wwm70ck1Aldr",
        "_Qb0UY-thCx5",
        "58AbwdkehJKO",
        "EuCL4KICk-hF"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}