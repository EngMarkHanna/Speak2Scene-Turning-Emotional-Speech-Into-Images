{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "bU88LkwqWuQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import gdown\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ],
      "metadata": {
        "id": "pVdSt8r63XAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset preparation"
      ],
      "metadata": {
        "id": "ThrZFyQ2XEv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Emoset dataset and prepare it for training. Extract relevant emotions and merge some that are needed. Create folders with images of different emotions and split new dataset into train/valid/test."
      ],
      "metadata": {
        "id": "aTwf3vcVXHZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O EmoSet-118K.zip \"https://www.dropbox.com/scl/fi/myue506itjfc06m7svdw6/EmoSet-118K.zip?rlkey=7f3oyjkr6zyndf0gau7t140rv&dl=1\""
      ],
      "metadata": {
        "id": "bCqX_qMbiHSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q -o \"/content/EmoSet-118K.zip\" -d \"/content/EmoSet-118K_extracted\""
      ],
      "metadata": {
        "id": "6_BcwYRfj63X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract images of different emotions\n",
        "KEEP_AWE_ANGER = True\n",
        "MAX_PER_CLASS = 10500\n",
        "\n",
        "# root folder\n",
        "ROOT = \"/content/EmoSet-118K_extracted\"\n",
        "ANNOT_DIR = os.path.join(ROOT, \"annotation\")\n",
        "IMAGE_DIR = os.path.join(ROOT, \"image\")\n",
        "\n",
        "# output folder\n",
        "!rm -rf \"/content/EmoSet_balanced\"\n",
        "OUT_DIR = \"/content/EmoSet_balanced\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def map_emotion(em):\n",
        "    mapping = {\n",
        "        \"anger\": \"angry\",\n",
        "        \"disgust\": \"disgust\",\n",
        "        \"fear\": \"fear\",\n",
        "        \"sadness\": \"sad\",\n",
        "        \"amusement\": \"happy\",\n",
        "        \"excitement\": \"happy\",\n",
        "    }\n",
        "\n",
        "    if KEEP_AWE_ANGER:\n",
        "        mapping[\"awe\"] = \"surprised\"\n",
        "        mapping[\"anger\"] = \"angry\"\n",
        "    else:\n",
        "        mapping[\"awe\"] = None\n",
        "        mapping[\"anger\"] = None\n",
        "\n",
        "    mapping[\"contentment\"] = None  # always excluded not relevant to our dataset\n",
        "\n",
        "    return mapping.get(em, None)\n",
        "\n",
        "\n",
        "#load all annottions\n",
        "all_records = []\n",
        "\n",
        "for emotion_folder in os.listdir(ANNOT_DIR):\n",
        "    full_emotion_dir = os.path.join(ANNOT_DIR, emotion_folder)\n",
        "\n",
        "    if not os.path.isdir(full_emotion_dir):\n",
        "        continue\n",
        "\n",
        "    for json_name in os.listdir(full_emotion_dir):\n",
        "        if not json_name.endswith(\".json\"):\n",
        "            continue\n",
        "\n",
        "        json_path = os.path.join(full_emotion_dir, json_name)\n",
        "\n",
        "        try:\n",
        "            with open(json_path, \"r\") as f:\n",
        "                entry = json.load(f)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        all_records.append(entry)\n",
        "\n",
        "print(\"Loaded annotation JSONs:\", len(all_records))\n",
        "\n",
        "\n",
        "#Group images by every target class\n",
        "grouped = {}\n",
        "\n",
        "for rec in all_records:\n",
        "    original_em = rec[\"emotion\"]\n",
        "    mapped = map_emotion(original_em)\n",
        "\n",
        "    if mapped is None:\n",
        "        continue\n",
        "\n",
        "    img_id = rec[\"image_id\"]\n",
        "    img_path = os.path.join(IMAGE_DIR, original_em, img_id + \".jpg\")\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        continue\n",
        "\n",
        "    grouped.setdefault(mapped, []).append(img_path)\n",
        "\n",
        "\n",
        "#Balance sampling when merging different emotions\n",
        "final_dataset = {}\n",
        "\n",
        "for cls, paths in grouped.items():\n",
        "\n",
        "    #Merge for happy\n",
        "    if cls == \"happy\":\n",
        "        amusement_paths  = [p for p in paths if \"/amusement/\"  in p]\n",
        "        excitement_paths = [p for p in paths if \"/excitement/\" in p]\n",
        "\n",
        "        print(f\"\\nHappy breakdown:\")\n",
        "        print(f\"  amusement:  {len(amusement_paths)}\")\n",
        "        print(f\"  excitement: {len(excitement_paths)}\")\n",
        "\n",
        "        half = MAX_PER_CLASS // 2\n",
        "\n",
        "        random.shuffle(amusement_paths)\n",
        "        random.shuffle(excitement_paths)\n",
        "\n",
        "        selected_amusement  = amusement_paths[:half]\n",
        "        selected_excitement = excitement_paths[:half]\n",
        "\n",
        "        selected = selected_amusement + selected_excitement\n",
        "        random.shuffle(selected)\n",
        "\n",
        "        final_dataset[cls] = selected\n",
        "        print(f\"happy: selected {len(selected)} balanced samples\\n\")\n",
        "        continue\n",
        "\n",
        "    #take randomely for other emotions\n",
        "    random.shuffle(paths)\n",
        "    selected = paths[:MAX_PER_CLASS]\n",
        "\n",
        "    final_dataset[cls] = selected\n",
        "    print(f\"{cls}: {len(selected)} images selected\")\n",
        "\n",
        "\n",
        "#Copy file to new dataset\n",
        "print(\"\\nCopying images into /content/EmoSet_balanced ...\\n\")\n",
        "\n",
        "for cls, paths in final_dataset.items():\n",
        "    out_dir = os.path.join(OUT_DIR, cls)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    for img_path in tqdm(paths, desc=f\"Copying {cls}\", total=len(paths)):\n",
        "        shutil.copy(img_path, os.path.join(out_dir, os.path.basename(img_path)))\n",
        "\n",
        "print(\"\\nBalanced dataset creation complete.\")\n",
        "print(\"Final dataset stored at:\", OUT_DIR)\n",
        "\n",
        "# Summary for each class\n",
        "print(\"\\nSummary:\")\n",
        "for cls, imgs in final_dataset.items():\n",
        "    print(f\"{cls}: {len(imgs)} images\")\n"
      ],
      "metadata": {
        "id": "VF4XlsFUnG5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split dataset ubti train/test/valid\n",
        "SRC = \"/content/EmoSet_balanced\"\n",
        "DEST = \"/content/dataset_new\"\n",
        "SPLITS = [\"train\", \"valid\", \"test\"]\n",
        "\n",
        "# Ratios\n",
        "train_ratio = 0.8\n",
        "val_ratio   = 0.1\n",
        "test_ratio  = 0.1\n",
        "\n",
        "# Make folders\n",
        "for split in SPLITS:\n",
        "    os.makedirs(os.path.join(DEST, split), exist_ok=True)\n",
        "\n",
        "# Process each emotion folder\n",
        "for emotion in os.listdir(SRC):\n",
        "    src_folder = os.path.join(SRC, emotion)\n",
        "    if not os.path.isdir(src_folder):\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nðŸ“ Processing class: {emotion}\")\n",
        "\n",
        "    # list and shuffle images\n",
        "    images = os.listdir(src_folder)\n",
        "    random.shuffle(images)\n",
        "\n",
        "    n = len(images)\n",
        "    n_train = int(n * train_ratio)\n",
        "    n_val   = int(n * val_ratio)\n",
        "\n",
        "    train_imgs = images[:n_train]\n",
        "    val_imgs   = images[n_train:n_train+n_val]\n",
        "    test_imgs  = images[n_train+n_val:]\n",
        "\n",
        "    # Create class folders\n",
        "    for split in SPLITS:\n",
        "        os.makedirs(os.path.join(DEST, split, emotion), exist_ok=True)\n",
        "\n",
        "    # Copy images\n",
        "    for img in tqdm(train_imgs, desc=f\"{emotion} â†’ train\"):\n",
        "        shutil.copy(os.path.join(src_folder, img),\n",
        "                    os.path.join(DEST, \"train\", emotion, img))\n",
        "\n",
        "    for img in tqdm(val_imgs, desc=f\"{emotion} â†’ valid\"):\n",
        "        shutil.copy(os.path.join(src_folder, img),\n",
        "                    os.path.join(DEST, \"valid\", emotion, img))\n",
        "\n",
        "    for img in tqdm(test_imgs, desc=f\"{emotion} â†’ test\"):\n",
        "        shutil.copy(os.path.join(src_folder, img),\n",
        "                    os.path.join(DEST, \"test\", emotion, img))\n",
        "\n",
        "print(f\"\\nSplit complete. Dataset ready at {DEST}\")"
      ],
      "metadata": {
        "id": "84ZaVBbEpk8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Model"
      ],
      "metadata": {
        "id": "mAtib4S1ZQ9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will apply transformations for best compatibility with model, setup data loaders, load model, and finally train it. While training we always check validation and test dataset."
      ],
      "metadata": {
        "id": "9cqRVrITZTty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform and setup test loaders\n",
        "base_dir = \"/content/dataset_new\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "val_dir   = os.path.join(base_dir, \"valid\")\n",
        "test_dir  = os.path.join(base_dir, \"test\")\n",
        "\n",
        "#Params for training\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "lr = 5e-5\n",
        "\n",
        "!mkdir \"/content/checkpoints\"\n",
        "ckpt_local_dir = \"/content/checkpoints\"\n",
        "os.makedirs(ckpt_local_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "SAVE_BEST_TEST = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Model: ViT-Base-16 (SWAG Weights) | Input Size: 384x384 | Batch: {batch_size}\")\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "train_ds = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "val_ds   = datasets.ImageFolder(val_dir,   transform=val_test_transform)\n",
        "test_ds  = datasets.ImageFolder(test_dir,  transform=val_test_transform)\n",
        "\n",
        "class_names = list(train_ds.class_to_idx.keys())\n",
        "num_classes = len(class_names)\n",
        "print(f\"âœ… Classes ({num_classes}):\", train_ds.class_to_idx)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(train_ds.classes, len(train_ds))\n",
        "print(val_ds.classes,   len(val_ds))\n",
        "print(test_ds.classes,  len(test_ds))"
      ],
      "metadata": {
        "id": "wiICr0QS3onY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup models, load pretrained weights, and change classification head.\n",
        "train_labels = [lbl for _, lbl in train_ds.samples]\n",
        "cls_w = compute_class_weight(\"balanced\", classes=np.unique(train_labels), y=train_labels)\n",
        "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(cls_w).to(device))\n",
        "\n",
        "print(\"Loading SWAG Weights\")\n",
        "weights = ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\n",
        "model = vit_b_16(weights=weights)\n",
        "\n",
        "in_features = model.heads.head.in_features\n",
        "model.heads.head = nn.Linear(in_features, num_classes)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "#Optimizer and Scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
      ],
      "metadata": {
        "id": "jvx1D97w3xpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and Evaluation\n",
        "@torch.no_grad()\n",
        "def evaluate(loader, name):\n",
        "    model.eval()\n",
        "    tot_loss = tot_correct = tot = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "\n",
        "        pred = out.argmax(1)\n",
        "        tot_loss += loss.item() * x.size(0)\n",
        "        tot_correct += (pred == y).sum().item()\n",
        "        tot += y.size(0)\n",
        "\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    avg_loss = tot_loss / tot if tot else 0.0\n",
        "    acc = tot_correct / tot if tot else 0.0\n",
        "\n",
        "    print(f\"{name} â†’ Loss: {avg_loss:.4f} | Acc: {acc:.4f}\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
        "    return avg_loss, acc\n",
        "\n",
        "def save_ckpt(path, epoch, tag):\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "        \"tag\": tag,\n",
        "    }, path)\n",
        "    print(f\"Saved Checkpoint: {path}\")\n",
        "\n",
        "\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_test_acc = 0.0\n",
        "\n",
        "print(\"\\nStart Training Loop...\")\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{epochs}\")\n",
        "\n",
        "    model.train()\n",
        "    run_loss = run_correct = run_total = 0\n",
        "\n",
        "    for x, y in tqdm(train_loader, desc=\"Training\"):\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        run_loss += loss.item() * x.size(0)\n",
        "        run_correct += (out.argmax(1) == y).sum().item()\n",
        "        run_total += x.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    train_acc = run_correct / run_total\n",
        "    print(f\"Train â†’ Loss: {run_loss/run_total:.4f} | Acc: {train_acc:.4f}\")\n",
        "\n",
        "\n",
        "    _, val_acc  = evaluate(val_loader,  \"Val\")\n",
        "    _, test_acc = evaluate(test_loader, \"Test (Monitoring)\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_local = os.path.join(ckpt_local_dir, \"vit_swag_best_val.pt\")\n",
        "\n",
        "        save_ckpt(best_local, epoch, tag=f\"best_val_acc_{best_val_acc:.4f}\")\n",
        "        print(f\"New Best Val Model ({best_val_acc:.4f}) saved to Drive.\")\n",
        "\n",
        "        with open(os.path.join(ckpt_local_dir, \"best_val_metrics.txt\"), \"w\") as f:\n",
        "            f.write(f\"epoch={epoch}, val_acc={best_val_acc:.6f}, test_acc={test_acc:.6f}\\n\")\n",
        "\n",
        "\n",
        "    if SAVE_BEST_TEST and test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_test_local = os.path.join(ckpt_local_dir, \"vit_swag_best_test_LEAKY.pt\")\n",
        "\n",
        "        save_ckpt(best_test_local, epoch, tag=f\"best_test_acc_{best_test_acc:.4f}\")\n",
        "        print(f\"New Best Test Model ({best_test_acc:.4f}) saved to Drive.\")\n",
        "\n",
        "print(\"\\nAll Process Complete!\")\n",
        "print(f\"Final Best Val Acc: {best_val_acc:.4f}\")\n",
        "if SAVE_BEST_TEST:\n",
        "    print(f\"Final Best Test Acc (Leaky): {best_test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "uFdcFMyt3pKV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}